var documenterSearchIndex = {"docs":
[{"location":"api/logging/","page":"Logging","title":"Logging","text":"CollapsedDocStrings = true","category":"page"},{"location":"api/logging/#Logging","page":"Logging","title":"Logging","text":"","category":"section"},{"location":"api/logging/","page":"Logging","title":"Logging","text":"Tsunami.log","category":"page"},{"location":"api/logging/#Tsunami.log","page":"Logging","title":"Tsunami.log","text":"Tsunami.log(trainer::Trainer, name::AbstractString, value; \n    [on_step, on_epoch, prog_bar, batchsize])\n\nLog a value with name name. Can be called from any function in the training loop or from a callback. Logs to the loggers specified in trainer.loggers.\n\nSee the Logging docs for more details.\n\nIf on_step is true, the value will be logged on each step. If on_epoch is true, the value will be accumulated and logged on each epoch. In this case, the default reduction is the mean over the batches, which will also take into account the batch size. If both on_step and on_epoch are true, the values will be logged as  \"<name>_step\" and \"<name>_epoch\"\n\nArguments\n\ntrainer::Trainer: The trainer object.\nname::AbstractString: The name of the value.\nvalue: The value to log.\nbatchsize: The size of the current batch. Used only when on_epoch == True             to compute the aggregate the batches. Defaults to trainer.fit_state.batchsize.\non_epoch::Bool: Whether to log the value on each epoch.                    Defaults to true if stage is :train_epoch_end or :val_epoch_end,                    false otherwise.\non_step::Bool: Whether to log the value on each step.                  Defaults to true if stage is :training, false for :validation and :testing.\nprog_bar::Bool: Whether to log the value to the progress bar. Defaults to true.\n\nExamples\n\nfunction val_step(model::Model, trainer, batch, batch_idx)\n    # log the validation loss\n    ...\n    Tsunami.log(trainer, \"val/loss\", val_loss)\nend\n\n\n\n\n\n","category":"function"},{"location":"api/logging/#Loggers","page":"Logging","title":"Loggers","text":"","category":"section"},{"location":"api/logging/#Tensorboard","page":"Logging","title":"Tensorboard","text":"","category":"section"},{"location":"api/logging/","page":"Logging","title":"Logging","text":"Tsunami.TensorBoardLogger\nTsunami.read_tensorboard_logs","category":"page"},{"location":"api/logging/#Tsunami.TensorBoardLogger","page":"Logging","title":"Tsunami.TensorBoardLogger","text":"TensorBoardLogger(run_dir)\n\nA logger that writes to writes tensorboard events to the run_dir directory. Relies on the TensorBoardLogger.jl package.\n\nSee also read_tensorboard_logs.\n\n\n\n\n\n","category":"type"},{"location":"api/logging/#Tsunami.read_tensorboard_logs","page":"Logging","title":"Tsunami.read_tensorboard_logs","text":"read_tensorboard_logs(logdir)\n\nReads all tensorboard events from the logdir path and returns them as a list of  (name, step, value) tuples.\n\nExample\n\njulia> Tsunami.fit!(model, trainer, train_dataloader);\n\njulia> events = Tsunami.read_tensorboard_logs(trainer.fit_state.run_dir)\n24-element Vector{Tuple{String, Int64, Any}}:\n(\"train/loss\", 1, 2.509954f0)\n (\"epoch\", 1, 1.0f0)\n (\"train/acc\", 1, 0.0f0)\n (\"train/loss\", 2, 2.2748244f0)\n (\"epoch\", 2, 1.0f0)\n (\"train/acc\", 2, 0.5f0)\n ...\n\n# Convert to a DataFrame\njulia> df = DataFrame([(; name, step, value) for (name, step, value) in events]);\n\njulia> unstack(df, :step, :name, :value)\n8×4 DataFrame\n Row │ step   train/loss  epoch     train/acc \n     │ Int64  Float32?    Float32?  Float32?  \n─────┼────────────────────────────────────────\n   1 │     1     2.50995       1.0   0.0\n   2 │     2     2.27482       1.0   0.5\n   3 │     3     2.06172       2.0   0.333333\n   4 │     4     1.72649       2.0   1.0\n   5 │     5     1.57971       3.0   1.0\n   6 │     6     1.39933       3.0   1.0\n   7 │     7     1.17671       4.0   1.0\n   8 │     8     1.17483       4.0   1.0\n\n\n\n\n\n","category":"function"},{"location":"api/foil/","page":"Foil","title":"Foil","text":"CollapsedDocStrings = true","category":"page"},{"location":"api/foil/#Foil","page":"Foil","title":"Foil","text":"","category":"section"},{"location":"api/foil/","page":"Foil","title":"Foil","text":"The Foil is a minimalistic version of the Trainer that allows to make only minimal changes to your Flux code while still obtaining many of the benefits of Tsunami. This is similar to what Lighting Fabric is to PyTorch Lightning. Foil also resembles HuggingFace's accelerate library.","category":"page"},{"location":"api/foil/","page":"Foil","title":"Foil","text":"Foil\nTsunami.setup","category":"page"},{"location":"api/foil/#Tsunami.Foil","page":"Foil","title":"Tsunami.Foil","text":"Foil(; kws...)\n\nA type that takes care of the acceleration of the training process.\n\nConstructor Arguments\n\naccelerator: Supports passing different accelerator types:\n:auto (default): Automatically select a gpu if available, otherwise fallback on cpu.\n:gpu: Like :auto, but will throw an error if no gpu is available.  In order for a gpu to be available, the corresponding package must be loaded (e.g. with using CUDA).  The trigger packages are CUDA.jl for Nvidia GPUs, AMDGPU.jl for AMD GPUs, and Metal.jl for Apple Silicon.\n:cpu: Force using the cpu.\nSee also the devices option.\ndevices: Pass an integer n to train on n devices (only 1 supported at the moment),   or a list of devices ids to train on specific devices (e.g. [2] to train on gpu with idx 2).   Ids indexing starts from 1. If nothing, will use the default device    (see MLDataDevices.gpu_device documentation).    Default: nothing.\nprecision: Supports passing different precision types (:bf16, :f16, :f32, :f64),    where :bf16 is BFloat16, :f16 is Float16, :f32 is Float32, and :f64 is Float64.   Default: :f32.\n\n\n\n\n\n","category":"type"},{"location":"api/foil/#Tsunami.setup","page":"Foil","title":"Tsunami.setup","text":"setup(foil::Foil, model, optimisers)\n\nSetup the model and optimisers for training sending them to the device and setting the precision. This function is called internally by Tsunami.fit!.\n\nSee also Foil.\n\n\n\n\n\n","category":"function"},{"location":"guides/#Guides","page":"How-To Guides","title":"Guides","text":"","category":"section"},{"location":"guides/#Selecting-a-GPU-backend","page":"How-To Guides","title":"Selecting a GPU backend","text":"","category":"section"},{"location":"guides/","page":"How-To Guides","title":"How-To Guides","text":"Tsunami supports both CPU and GPU devices. To select a device, use the accelerator and devices keyword arguments in the Trainer constructor.","category":"page"},{"location":"guides/","page":"How-To Guides","title":"How-To Guides","text":"trainer = Trainer(accelerator = :auto) # default, selects CPU or GPU depending on availability\ntrainer = Trainer(accelerator = :gpu) # forces selection to GPU, errors if no GPU is available","category":"page"},{"location":"guides/","page":"How-To Guides","title":"How-To Guides","text":"Currently supported accelerators are :auto, :gpu, and :cpu. See the Trainer documentation for more details.","category":"page"},{"location":"guides/","page":"How-To Guides","title":"How-To Guides","text":"By default, Tsunami will use the first of the available GPUs and the CPU if no GPUs are present.  To select a specific GPU, use the devices keyword argument:","category":"page"},{"location":"guides/","page":"How-To Guides","title":"How-To Guides","text":"trainer = Trainer(devices = [1])","category":"page"},{"location":"guides/","page":"How-To Guides","title":"How-To Guides","text":"Devices are indexed starting from 1, as in the MLDataDevices.gpu_device method used by Flux.","category":"page"},{"location":"guides/#Selecting-an-automatic-differentiation-engine","page":"How-To Guides","title":"Selecting an automatic differentiation engine","text":"","category":"section"},{"location":"guides/","page":"How-To Guides","title":"How-To Guides","text":"Zygote is the default Automatic Differentiation (AD) engine in Tsunami, used for computing gradients during training. Enzyme is an alternative AD engine that can sometimes provide faster performance and differentiate through mutating functions.","category":"page"},{"location":"guides/","page":"How-To Guides","title":"How-To Guides","text":"To select an AD engine, use the autodiff keyword argument in the Trainer constructor:","category":"page"},{"location":"guides/","page":"How-To Guides","title":"How-To Guides","text":"trainer = Trainer(autodiff = :enzyme) # options are :zygote (default) and :enzyme","category":"page"},{"location":"guides/#Gradient-accumulation","page":"How-To Guides","title":"Gradient accumulation","text":"","category":"section"},{"location":"guides/","page":"How-To Guides","title":"How-To Guides","text":"Gradient accumulation is a technique that allows you to simulate larger batch sizes by accumulating gradients over multiple batches. This is useful when you want to use a large batch size but your GPU does not have enough memory.","category":"page"},{"location":"guides/","page":"How-To Guides","title":"How-To Guides","text":"Optimisers.jl supports gradient accumulation the AccumGrad rule:","category":"page"},{"location":"guides/","page":"How-To Guides","title":"How-To Guides","text":"    AccumGrad(n::Int)\n\nA rule constructed `OptimiserChain(AccumGrad(n), Rule())` will accumulate for `n` steps, before applying Rule to the mean of these `n` gradients.\n\nThis is useful for training with effective batch sizes too large for the available memory. Instead of computing the gradient for batch size `b` at once, compute it for size `b/n` and accumulate `n` such gradients.","category":"page"},{"location":"guides/","page":"How-To Guides","title":"How-To Guides","text":"AccumGrad can be easily integrated into Tsunami's configure_optimisers:","category":"page"},{"location":"guides/","page":"How-To Guides","title":"How-To Guides","text":"using Optimisers\n\nfunction Tsunami.configure_optimisers(model::Model, trainer)\n    opt = OptimiserChain(AccumGrad(5), AdamW(1e-3))\n    opt_state = Optimiser.setup(opt, model)\n    return opt_state\nend","category":"page"},{"location":"guides/#Gradient-clipping","page":"How-To Guides","title":"Gradient clipping","text":"","category":"section"},{"location":"guides/","page":"How-To Guides","title":"How-To Guides","text":"Gradient clipping is a technique that allows you to limit the range or the norm of the gradients. This is useful to prevent exploding gradients and improve training stability.","category":"page"},{"location":"guides/","page":"How-To Guides","title":"How-To Guides","text":"Optimisers.jl supports gradient clipping with the ClipNorm and ClipGrad rule:","category":"page"},{"location":"guides/","page":"How-To Guides","title":"How-To Guides","text":"    ClipGrad(δ = 10f0)\n\nRestricts every gradient component to obey -δ ≤ dx[i] ≤ δ.","category":"page"},{"location":"guides/","page":"How-To Guides","title":"How-To Guides","text":"    ClipNorm(ω = 10f0, p = 2; throw = true)\n\nScales any gradient array for which norm(dx, p) > ω to stay at this threshold (unless p==0).\nThrows an error if the norm is infinite or NaN, which you can turn off with throw = false.","category":"page"},{"location":"guides/","page":"How-To Guides","title":"How-To Guides","text":"Gradient clipping can be easily integrated into Tsunami's configure_optimisers:","category":"page"},{"location":"guides/","page":"How-To Guides","title":"How-To Guides","text":"using Optimisers\n\nfunction Tsunami.configure_optimisers(model::Model, trainer)\n    opt = OptimiserChain(ClipGrad(0.1), AdamW(1e-3))\n    opt_state = Optimiser.setup(opt, model)\n    return opt_state\nend","category":"page"},{"location":"api/hooks/","page":"Hooks","title":"Hooks","text":"CollapsedDocStrings = true","category":"page"},{"location":"api/hooks/#Hooks","page":"Hooks","title":"Hooks","text":"","category":"section"},{"location":"api/hooks/","page":"Hooks","title":"Hooks","text":"Hooks are a way to extend the functionality of Tsunami. They are a way to inject custom code into the FluxModule or into a Callback at various points in the training, testing, and validation loops.","category":"page"},{"location":"api/hooks/","page":"Hooks","title":"Hooks","text":"At a high level, and omitting function imputs and outputs, a simplified version of the Tsunami.fit! method looks like this:","category":"page"},{"location":"api/hooks/","page":"Hooks","title":"Hooks","text":"function fit!()\n    configure_optimizers()\n    \n    for epoch in epochs\n        train_loop()\n    end\nend\n\nfunction train_loop()\n    on_train_epoch_start()\n    set_learning_rate(lr_scheduler, epoch)\n\n    for (batch, batch_idx) in enumerate(train_dataloader)\n        batch = transfer_batch_to_device(batch)\n        on_train_batch_start(batch, batch_idx)\n        out, grad = out_and_gradient(train_step, model, trainer, batch, batch_idx)\n        on_before_update(out, grad)\n        update!(opt_state, model, grad)\n        on_train_batch_end(out, batch, batch_idx)\n        if should_check_val\n            val_loop()\n        end\n    end\n    on_train_epoch_end()\nend\n\nfunction val_loop()\n    on_val_epoch_start()\n    for (batch, batch_idx) in val_dataloader\n        batch = transfer_batch_to_device(batch)\n        on_val_batch_start(batch, batch_idx)\n        out = val_step(model, trainer, batch, batch_idx)\n        on_val_batch_end(out, batch, batch_idx)\n    end\n    on_val_epoch_end()\nend","category":"page"},{"location":"api/hooks/","page":"Hooks","title":"Hooks","text":"Each on_something hook takes as input the model and the trainer.","category":"page"},{"location":"api/hooks/#Hooks-API","page":"Hooks","title":"Hooks API","text":"","category":"section"},{"location":"api/hooks/","page":"Hooks","title":"Hooks","text":"Tsunami.on_before_update\nTsunami.on_train_batch_start\nTsunami.on_train_batch_end\nTsunami.on_train_epoch_start\nTsunami.on_train_epoch_end\nTsunami.on_test_batch_start\nTsunami.on_test_batch_end\nTsunami.on_test_epoch_start\nTsunami.on_test_epoch_end\nTsunami.on_val_batch_start\nTsunami.on_val_batch_end\nTsunami.on_val_epoch_start\nTsunami.on_val_epoch_end","category":"page"},{"location":"api/hooks/#Tsunami.on_before_update","page":"Hooks","title":"Tsunami.on_before_update","text":"on_before_update([callback,] model, trainer, out, grad)\n\nCalled before the call to Optimisers.update! that  applies the gradient grad to update the model's parameters. out is the output of the last call to train_step.\n\n\n\n\n\n","category":"function"},{"location":"api/hooks/#Tsunami.on_train_batch_start","page":"Hooks","title":"Tsunami.on_train_batch_start","text":"on_train_batch_start([callback,] model, trainer, batch, batch_idx)\n\nCalled at the beginning of each training batch.\n\n\n\n\n\n","category":"function"},{"location":"api/hooks/#Tsunami.on_train_batch_end","page":"Hooks","title":"Tsunami.on_train_batch_end","text":"on_train_batch_end([callback,] model, trainer, out, batch, batch_idx)\n\nCalled at the end of each iteration in the training loop. out is the output of train_step.\n\n\n\n\n\n","category":"function"},{"location":"api/hooks/#Tsunami.on_train_epoch_start","page":"Hooks","title":"Tsunami.on_train_epoch_start","text":"on_train_epoch_start([callback,] model, trainer)\n\nCalled at the beginning of each training epoch.\n\n\n\n\n\n","category":"function"},{"location":"api/hooks/#Tsunami.on_train_epoch_end","page":"Hooks","title":"Tsunami.on_train_epoch_end","text":"on_train_epoch_end([callback,] model, trainer)\n\nCalled at the end of each training epoch.\n\nTo access all batch outputs at the end of the epoch,  you can cache step outputs as an attribute of the model and access them in this hook:\n\nSee also on_train_epoch_start.\n\nExamples\n\nstruct Callback\n    training_step_outputs::Vector{Float32}\n    # other fields...\nend\n\nfunction Tsunami.train_step(model::MyModel, trainer, batch)\n    ...\n    return (loss = loss, accuracy = accuracy)\nend\n\nfunction Tsunami.on_train_epoch_start(cb::Callback, model, trainer)\n    empty!(cb.training_step_outputs)\nend\n\nfunction Tsunami.on_train_batch_end(cb::Callback, model, trainer, out, batch, batch_idx)\n    push!(cb.training_step_outputs, out.accuracy)\nend\n\nfunction Tsunami.on_train_epoch_end(cb::Callback, model, trainer)\n    println(\"Mean accuracy: \", mean(cb.training_step_outputs))\nend\n\n\n\n\n\n","category":"function"},{"location":"api/hooks/#Tsunami.on_test_batch_start","page":"Hooks","title":"Tsunami.on_test_batch_start","text":"on_test_batch_start([callback,] model, trainer, batch, batch_idx)\n\nCalled at the beginning of each test batch.\n\n\n\n\n\n","category":"function"},{"location":"api/hooks/#Tsunami.on_test_batch_end","page":"Hooks","title":"Tsunami.on_test_batch_end","text":"on_test_batch_end([callback,] model, trainer, out, batch, batch_idx)\n\nCalled at the end of each iteration in the test loop. out is the output of test_step.\n\n\n\n\n\n","category":"function"},{"location":"api/hooks/#Tsunami.on_test_epoch_start","page":"Hooks","title":"Tsunami.on_test_epoch_start","text":"on_test_epoch_start([callback,] model, trainer)\n\nCalled at the beginning of each test epoch.\n\n\n\n\n\n","category":"function"},{"location":"api/hooks/#Tsunami.on_test_epoch_end","page":"Hooks","title":"Tsunami.on_test_epoch_end","text":"on_test_epoch_end([callback,] model, trainer)\n\nCalled at the end of each test epoch.\n\n\n\n\n\n","category":"function"},{"location":"api/hooks/#Tsunami.on_val_batch_start","page":"Hooks","title":"Tsunami.on_val_batch_start","text":"on_val_batch_start([callback,] model, trainer, batch, batch_idx)\n\nCalled at the beginning of each validation batch.\n\n\n\n\n\n","category":"function"},{"location":"api/hooks/#Tsunami.on_val_batch_end","page":"Hooks","title":"Tsunami.on_val_batch_end","text":"on_val_batch_end([callback,] model, trainer, out, batch, batch_idx)\n\nCalled at the end of each iteration in the validation loop. out is the output of val_step.\n\n\n\n\n\n","category":"function"},{"location":"api/hooks/#Tsunami.on_val_epoch_start","page":"Hooks","title":"Tsunami.on_val_epoch_start","text":"on_val_epoch_start([callback,] model, trainer)\n\nCalled  at the beginning of each validation epoch.\n\n\n\n\n\n","category":"function"},{"location":"api/hooks/#Tsunami.on_val_epoch_end","page":"Hooks","title":"Tsunami.on_val_epoch_end","text":"on_val_epoch_end([callback,] model, trainer)\n\nCalled at the end of each validation epoch.\n\n\n\n\n\n","category":"function"},{"location":"api/fluxmodule/","page":"FluxModule","title":"FluxModule","text":"CollapsedDocStrings = true","category":"page"},{"location":"api/fluxmodule/#sec_fluxmodule","page":"FluxModule","title":"FluxModule","text":"","category":"section"},{"location":"api/fluxmodule/","page":"FluxModule","title":"FluxModule","text":"The FluxModule abstract type is the entry point for defining custom models in Tsunami. Subtypes of FluxModule are designed to be used with the Tsunami.fit! method, but can also be used independently.","category":"page"},{"location":"api/fluxmodule/","page":"FluxModule","title":"FluxModule","text":"FluxModule","category":"page"},{"location":"api/fluxmodule/#Tsunami.FluxModule","page":"FluxModule","title":"Tsunami.FluxModule","text":"abstract type FluxModule end\n\nAn abstract type for Flux models. A FluxModule helps orgainising you code and provides a standard interface for training.\n\nA FluxModule comes with the functionality provided by Flux.@layer  (pretty printing, etc...) and the ability to interact with  Trainer and Optimisers.jl.\n\nYou can change the trainables by implementing Optimisers.trainables.\n\nTypes subtyping from FluxModule have to implement the following methods  in order to interact with a Trainer.\n\nRequired methods\n\nconfigure_optimisers(model, trainer).\ntrain_step(model, trainer, batch, [batch_idx]).\n\nOptional Methods\n\nval_step(model, trainer, batch, [batch_idx]).\ntest_step(model, trainer, batch, [batch_idx]).\ngeneric hooks.\n\nExamples\n\nusing Flux, Tsunami, Optimisers\n\n# Define a Multilayer Perceptron implementing the FluxModule interface\n\nstruct Model <: FluxModule\n    net\nend\n\nfunction Model()\n    net = Chain(Dense(4 => 32, relu), Dense(32 => 2))\n    return Model(net)\nend\n\n(model::Model)(x) = model.net(x)\n\nfunction Tsunami.train_step(model::Model, trainer, batch)\n    x, y = batch\n    y_hat = model(x)\n    loss = Flux.Losses.mse(y_hat, y)\n    return loss\nend\n\nfunction Tsunami.configure_optimisers(model::Model, trainer)\n    return Optimisers.setup(Optimisers.Adam(1f-3), model)\nend\n\n# Prepare the dataset and the DataLoader\nX, Y = rand(4, 100), rand(2, 100)\ntrain_dataloader = Flux.DataLoader((X, Y), batchsize=10)\n\n# Create and Train the model\nmodel = Model()\ntrainer = Trainer(max_epochs=10)\nTsunami.fit!(model, trainer, train_dataloader)\n\n\n\n\n\n","category":"type"},{"location":"api/fluxmodule/#Required-methods","page":"FluxModule","title":"Required methods","text":"","category":"section"},{"location":"api/fluxmodule/","page":"FluxModule","title":"FluxModule","text":"The following methods must be implemented for a subtype of FluxModule to be used with Tsunami.","category":"page"},{"location":"api/fluxmodule/","page":"FluxModule","title":"FluxModule","text":"Tsunami.configure_optimisers\nTsunami.train_step","category":"page"},{"location":"api/fluxmodule/#Tsunami.configure_optimisers","page":"FluxModule","title":"Tsunami.configure_optimisers","text":"configure_optimisers(model, trainer)\n\nReturn an optimiser's state initialized for the model. It can also return a tuple of (optimiser, scheduler), where scheduler is any callable object that takes  the current epoch as input and returns a scalar that will be  set as the learning rate for the next epoch.\n\nExamples\n\nusing Optimisers, ParameterSchedulers\n\nfunction Tsunami.configure_optimisers(model::Model, trainer)\n    return Optimisers.setup(AdamW(1f-3), model)\nend\n\n# Now with a scheduler dropping the learning rate by a factor 10 \n# at epochs [50, 100, 200] starting from the initial value of 1e-2\nfunction Tsunami.configure_optimisers(model::Model, trainer)\n\n    function lr_scheduler(epoch)\n        if epoch <= 50\n            return 1e-2\n        elseif epoch <= 100\n            return 1e-3\n        elseif epoch <= 200\n            return 1e-4\n        else\n            return 1e-5\n        end\n    end\n    \n    opt_state = Optimisers.setup(AdamW(), model)\n    return opt_state, lr_scheduler\nend\n\n# Same as above but using the ParameterSchedulers package.\nfunction Tsunami.configure_optimisers(model::Model, trainer)\n    lr_scheduler = ParameterSchedulers.Step(1f-2, 0.1f0, [50, 50, 100])\n    opt_state = Optimisers.setup(AdamW(), model)\n    return opt_state, lr_scheduler\nend\n\n\n\n\n\n","category":"function"},{"location":"api/fluxmodule/#Tsunami.train_step","page":"FluxModule","title":"Tsunami.train_step","text":"train_step(model, trainer, batch, [batch_idx])\n\nThe method called at each training step during Tsunami.fit!. It should compute the forward pass of the model and return the loss  (a scalar) corresponding to the minibatch batch.  The optional argument batch_idx is the index of the batch in the current epoch.\n\nAny Model <: FluxModule should implement either  train_step(model::Model, trainer, batch) or train_step(model::Model, trainer, batch, batch_idx).\n\nThe training loop in Tsunami.fit! approximately looks like this:\n\nfor epoch in 1:epochs\n    for (batch_idx, batch) in enumerate(train_dataloader)\n        grads = gradient(model) do m\n            loss = train_step(m, trainer, batch, batch_idx)\n            return loss\n        end\n        Optimisers.update!(opt, model, grads[1])\n    end\nend\n\nThe output can be either a scalar or a named tuple:\n\nIf a scalar is returned, it is assumed to be the loss.\nIf a named tuple is returned, it has to contain the loss field. \n\nThe output can be accessed in hooks such as on_before_update or on_train_batch_end.\n\nExamples\n\nfunction Tsunami.train_step(model::Model, trainer, batch)\n    x, y = batch\n    ŷ = model(x)\n    loss = Flux.Losses.logitcrossentropy(ŷ, y)\n    Tsunami.log(trainer, \"loss/train\", loss)\n    Tsunami.log(trainer, \"accuracy/train\", Tsunami.accuracy(ŷ, y))\n    return loss\nend\n\n\n\n\n\n","category":"function"},{"location":"api/fluxmodule/#Optional-methods","page":"FluxModule","title":"Optional methods","text":"","category":"section"},{"location":"api/fluxmodule/","page":"FluxModule","title":"FluxModule","text":"The following methods have default implementations that can be overridden if necessary. See also the Hooks section of the documentation for other methods that can be overridden.","category":"page"},{"location":"api/fluxmodule/","page":"FluxModule","title":"FluxModule","text":"Tsunami.val_step\nTsunami.test_step","category":"page"},{"location":"api/fluxmodule/#Tsunami.val_step","page":"FluxModule","title":"Tsunami.val_step","text":"val_step(model, trainer, batch, [batch_idx])\n\nThe method called at each validation step during Tsunami.fit!. Tipically used for computing metrics and statistics on the validation  batch batch. The optional argument batch_idx is the index of the batch in the current  validation epoch. \n\nA Model <: FluxModule should implement either  val_step(model::Model, trainer, batch) or val_step(model::Model, trainer, batch, batch_idx).\n\nOptionally, the method can return a scalar or a named tuple, to be used in hooks such as  on_val_batch_end.\n\nSee also train_step.\n\nExamples\n\nfunction Tsunami.val_step(model::Model, trainer, batch)\n    x, y = batch\n    ŷ = model(x)\n    loss = Flux.Losses.logitcrossentropy(ŷ, y)\n    accuracy = Tsunami.accuracy(ŷ, y)\n    Tsunami.log(trainer, \"loss/val\", loss, on_step = false, on_epoch = true)\n    Tsunami.log(trainer, \"loss/accuracy\", accuracy, on_step = false, on_epoch = true)\nend\n\n\n\n\n\n","category":"function"},{"location":"api/fluxmodule/#Tsunami.test_step","page":"FluxModule","title":"Tsunami.test_step","text":"test_step(model, trainer, batch, [batch_idx])\n\nSimilard to val_step but called at each test step.\n\n\n\n\n\n","category":"function"},{"location":"api/utils/","page":"Utils","title":"Utils","text":"CollapsedDocStrings = true","category":"page"},{"location":"api/utils/#Utils","page":"Utils","title":"Utils","text":"","category":"section"},{"location":"api/utils/","page":"Utils","title":"Utils","text":"Tsunami provides some utility functions to make your life easier.","category":"page"},{"location":"api/utils/","page":"Utils","title":"Utils","text":"Tsunami.accuracy\nTsunami.seed!\nTsunami.foreach_trainable","category":"page"},{"location":"api/utils/#Tsunami.accuracy","page":"Utils","title":"Tsunami.accuracy","text":"accuracy(ŷ::AbstractMatrix, y)\n\nCompute the classification accuracy of a batch of predictions ŷ against true labels y. y can be either a vector or a matrix.  If y is a vector, it is assumed that the labels are integers in the range 1:K  where K == size(ŷ, 1) is the number of classes.\n\n\n\n\n\n","category":"function"},{"location":"api/utils/#Tsunami.seed!","page":"Utils","title":"Tsunami.seed!","text":"seed!(seed::Int)\n\nSeed the RNGs of both CPU and GPU.\n\n\n\n\n\n","category":"function"},{"location":"api/utils/#Tsunami.foreach_trainable","page":"Utils","title":"Tsunami.foreach_trainable","text":"foreach_trainable(f, x, ys...)\n\nApply f to each trainable array in object x (or x itself if it is a leaf array) recursing into the children given by Optimisers.trainable.\n\nys are optional additional objects with the same structure as x. f will be applied to corresponding elements of x and ys.\n\n\n\n\n\n","category":"function"},{"location":"api/trainer/","page":"Trainer","title":"Trainer","text":"CollapsedDocStrings = true","category":"page"},{"location":"api/trainer/#Trainer","page":"Trainer","title":"Trainer","text":"","category":"section"},{"location":"api/trainer/","page":"Trainer","title":"Trainer","text":"The Trainer struct is the main entry point for training a model. It is responsible for managing the training loop, logging, and checkpointing. It is also responsible for managing the FitState struct, which contains the state of the training loop. ","category":"page"},{"location":"api/trainer/","page":"Trainer","title":"Trainer","text":"Pass a model (a FluxModule) and a trainer to the function Tsunami.fit! to train the model. After training, you can use the Tsunami.test function to test the model on a test dataset.","category":"page"},{"location":"api/trainer/","page":"Trainer","title":"Trainer","text":"Tsunami.Trainer\nTsunami.fit!\nTsunami.FitState\nTsunami.test\nTsunami.validate","category":"page"},{"location":"api/trainer/#Tsunami.Trainer","page":"Trainer","title":"Tsunami.Trainer","text":"Trainer(; kws...)\n\nA type storing the training options to be passed to fit!.\n\nA Trainer object also contains a field fit_state of type FitState mantaining updated information about  the fit state during the execution of fit!.\n\nConstructor Arguments\n\nautodiff: The automatic differentiation engine to use.                Possible values are :zygote and :enzyme . Default: :zygote.\ncallbacks: Pass a single or a list of callbacks. Default nothing.\ncheckpointer: If true, enable checkpointing.                   Default: true.\ndefault_root_dir : Default path for logs and weights.                     Default: pwd().\nfast_dev_run: If set to true runs a single batch for train and validation to find any bugs.             Default: false.\nlog_every_n_steps: How often to log within steps. See also logger.            Default: 50.\nlogger: If true use tensorboard for logging.           Every output of the train_step will be logged every 50 steps by default.           Set log_every_n_steps to change this.           Default: true.\nmax_epochs: Stop training once this number of epochs is reached.                Disabled by default (nothing).                If both max_epochs and max_steps are not specified,                defaults to max_epochs = 1000. To enable infinite training, set max_epochs = -1.               Default: nothing.\nmax_steps: Stop training after this number of steps.               Disabled by default (-1).               If max_steps = -1 and max_epochs = nothing, will default to max_epochs = 1000.               To enable infinite training, set max_epochs to -1.              Default: -1.\nprogress_bar: It true, shows a progress bar during training.                  Default: true.\nval_every_n_epochs: Perform a validation loop every after every N training epochs.                       The validation loop is in any case performed at the end of the last training epoch.                       Set to 0 or negative to disable validation.                       Default: 1.\n\nThe constructor also take any of the Foil's constructor arguments:\n\naccelerator: Supports passing different accelerator types:\n:auto (default): Automatically select a gpu if available, otherwise fallback on cpu.\n:gpu: Like :auto, but will throw an error if no gpu is available.  In order for a gpu to be available, the corresponding package must be loaded (e.g. with using CUDA).  The trigger packages are CUDA.jl for Nvidia GPUs, AMDGPU.jl for AMD GPUs, and Metal.jl for Apple Silicon.\n:cpu: Force using the cpu.\nSee also the devices option.\ndevices: Pass an integer n to train on n devices (only 1 supported at the moment),   or a list of devices ids to train on specific devices (e.g. [2] to train on gpu with idx 2).   Ids indexing starts from 1. If nothing, will use the default device    (see MLDataDevices.gpu_device documentation).    Default: nothing.\nprecision: Supports passing different precision types (:bf16, :f16, :f32, :f64),    where :bf16 is BFloat16, :f16 is Float16, :f32 is Float32, and :f64 is Float64.   Default: :f32.\n\nFields\n\nBesides most of the constructor arguments, a Trainer object also contains the following fields:\n\nfit_state: A FitState object storing the state of execution during a call to fit!.\nfoil: A Foil object.\nloggers: A list of loggers.\nlr_schedulers: The learning rate schedulers used for training.\noptimisers: The optimisers used for training.\n\nExamples\n\ntrainer = Trainer(max_epochs = 10, \n                  accelerator = :cpu,\n                  checkpointer = true,\n                  logger = true)\n\nTsunami.fit!(model, trainer, train_dataloader, val_dataloader)\n\n\n\n\n\n","category":"type"},{"location":"api/trainer/#Tsunami.fit!","page":"Trainer","title":"Tsunami.fit!","text":"fit!(model, trainer, train_dataloader, [val_dataloader]; [ckpt_path])\n\nTrain model using the configuration given by trainer. If ckpt_path is given, training is resumed from the checkpoint.\n\nAfter the fit, trainer.fit_state will contain the final state of the training.\n\nSee also Trainer and FitState.\n\nArguments\n\nmodel: A Flux model subtyping FluxModule.\ntrainer: A Trainer object storing the configuration options for fit!.\ntrain_dataloader: An iterator over the training dataset, typically a Flux.DataLoader.\nval_dataloader: An iterator over the validation dataset, typically a Flux.DataLoader. Default: nothing.\nckpt_path: Path of the checkpoint from which training is resumed. Default: nothing.\n\nExamples\n\nmodel = ...\ntrainer = Trainer(max_epochs = 10)\nTsunami.fit!(model, trainer, train_dataloader, val_dataloader)\nrun_dir = trainer.fit_state.run_dir\n\n# Resume training from checkpoint\ntrainer = Trainer(max_epochs = 20) # train for 10 more epochs\nckpt_path = joinpath(run_dir, \"checkpoints\", \"ckpt_last.jld2\")\nfit_state′ = Tsunami.fit!(model, trainer, train_dataloader, val_dataloader; ckpt_path)\n\n\n\n\n\n","category":"function"},{"location":"api/trainer/#Tsunami.FitState","page":"Trainer","title":"Tsunami.FitState","text":"FitState\n\nA type storing the state of execution during a call to fit!. \n\nA FitState object is part of a Trainer object.\n\nFields\n\nepoch: the current epoch number.\nrun_dir: the directory where the logs and checkpoints are saved.\nstage: the current stage of execution. One of :training, :train_epoch_end, :validation, :val_epoch_end.\nstep: the current step number.\nbatchsize: number of samples in the current batch.\nshould_stop: set to true to stop the training loop.\n\n\n\n\n\n","category":"type"},{"location":"api/trainer/#Tsunami.test","page":"Trainer","title":"Tsunami.test","text":"test(model::FluxModule, trainer, dataloader)\n\nRun the test loop, calling the test_step method on the model for each batch returned by the dataloader. Returns the aggregated results from the values logged in the test_step as a dictionary.\n\nExamples\n\njulia> struct Model <: FluxModule end \n\njulia> function Tsunami.test_step(::Model, trainer, batch)\n    Tsunami.log(trainer, \"test/loss\", rand())\nend\n\njulia> model, trainer = Model(), Trainer();\n\njulia> test_results = Tsunami.test(model, trainer, [rand(2) for i=1:3]);\nTesting: 100%|████████████████████████████████████████████████████████████████████████████████| Time: 0:00:00 (6.04 μs/it)\n  test/loss:  0.675\n\njulia> test_results\nDict{String, Float64} with 1 entry:\n  \"test/loss\" => 0.674665\n\n\n\n\n\n","category":"function"},{"location":"api/trainer/#Tsunami.validate","page":"Trainer","title":"Tsunami.validate","text":"validate(model::FluxModule, trainer, dataloader)\n\nRun the validation loop, calling the val_step method on the model for each batch returned by the dataloader. Returns the aggregated results from the values logged in the val_step as a dictionary.\n\nSee also Tsunami.test and Tsunami.fit!.\n\n\n\n\n\n","category":"function"},{"location":"","page":"Home","title":"Home","text":"<img align=\"right\" width=\"200px\" src=\"https://raw.githubusercontent.com/CarloLucibello/Tsunami.jl/main/docs/src/assets/thegreatwave.jpg\">","category":"page"},{"location":"#Tsunami.jl","page":"Home","title":"Tsunami.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"(Image: ) (Image: ) (Image: codecov)","category":"page"},{"location":"","page":"Home","title":"Home","text":"A high-level deep learning framework for the Julia language that helps you focus and organize the relevant part of your code while removing the boilerplate. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"Tsunami  is built on top of Flux.jl and it is heavily inspired by pytorch-lightning (although LightningAI is not involved in this project).","category":"page"},{"location":"#Features","page":"Home","title":"Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Use Tsunami.fit! instead of implementing a training loop.\nLogging (tensorboard).\nCheckpoints (save and resume training).\nHyperparameters' schedulers.\nCUDA, AMDGPU, and Metal GPU support.\nProgress bars.\nNice organization of your code.\nAutomatic Differentiation through Zygote or Enzyme.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Install Tsunami using the Julia package manager:","category":"page"},{"location":"","page":"Home","title":"Home","text":"pkg> add Tsunami","category":"page"},{"location":"#Usage","page":"Home","title":"Usage","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Define your model by subtyping the FluxModule abstract type, implement a few required methods, then let the Trainer train the model on your dataset with Tsunami.fit!. Tsunami will handle the boilerplate (training loop, logging, gpu movement, validation, ...).","category":"page"},{"location":"","page":"Home","title":"Home","text":"In the following script, we train a Multilayer Perceptron on the FashionMNIST dataset using Tsunami:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Flux, Optimisers, Statistics, Tsunami, MLDatasets\nusing MLUtils: DataLoader, flatten, mapobs\n## uncomment one of the following for GPU acceleration\n# using CUDA\n# using AMDGPU\n# using Metal\n\n## Define the model \n\nstruct MLP{T} <: FluxModule\n    net::T\nend\n\nMLP() = MLP(Chain(Dense(28^2 => 512, relu), Dense(512 => 10)))\n\n(model::MLP)(x) = model.net(flatten(x))\n\nfunction loss_and_accuracy(model::MLP, batch)\n    x, y = batch\n    ŷ = model(x)\n    return Flux.logitcrossentropy(ŷ, y), Tsunami.accuracy(ŷ, y)\nend\n\nfunction Tsunami.train_step(model::MLP, trainer, batch)\n    loss, acc = loss_and_accuracy(model, batch)\n    Tsunami.log(trainer, \"loss/train\", loss, prog_bar=true)\n    Tsunami.log(trainer, \"accuracy/train\", acc, prog_bar=true)\n    return loss\nend\n\nfunction Tsunami.val_step(model::MLP, trainer, batch)\n    loss, acc = loss_and_accuracy(model, batch)\n    Tsunami.log(trainer, \"loss/val\", loss)\n    Tsunami.log(trainer, \"accuracy/val\", acc)\nend\n\nTsunami.configure_optimisers(model::MLP, trainer) = \n    Optimisers.setup(Optimisers.AdamW(1e-3), model)\n\n## Prepare the data\n\nfunction mnist_transform(batch)\n    x, y = batch\n    y = Flux.onehotbatch(y, 0:9)\n    return (x, y)\nend\n\ntrain_data = FashionMNIST(split=:train)\ntrain_data = mapobs(mnist_transform, train_data)[:]\ntrain_loader = DataLoader(train_data, batchsize=128, shuffle=true)\n\ntest_data = FashionMNIST(split=:test)\ntest_data = mapobs(mnist_transform, test_data)[:]\ntest_loader = DataLoader(test_data, batchsize=128)\n\n## Create and train the model\n\nmodel = MLP()\ntrainer = Trainer(max_epochs=5)\nTsunami.fit!(model, trainer, train_loader, test_loader)","category":"page"},{"location":"","page":"Home","title":"Home","text":"What follows is the final output of the script. The script will train the model on GPU if available and will also write tensorboard logs and and model checkpoints to disk.","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: )","category":"page"},{"location":"","page":"Home","title":"Home","text":"See the documentation and check the examples folder to learn more.","category":"page"},{"location":"#Contributions-are-welcome!","page":"Home","title":"Contributions are welcome!","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you want to contribute to Tsunami, please open an issue or a pull request. Any help is appreciated!","category":"page"},{"location":"#Similar-julia-libraries","page":"Home","title":"Similar julia libraries","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"FastAI.jl\nFluxTraining.jl\nIgnite.jl","category":"page"},{"location":"api/callbacks/","page":"Callbacks","title":"Callbacks","text":"CollapsedDocStrings = true","category":"page"},{"location":"api/callbacks/#Callbacks","page":"Callbacks","title":"Callbacks","text":"","category":"section"},{"location":"api/callbacks/","page":"Callbacks","title":"Callbacks","text":"Callbacks are functions that are called at certain points in the training process. They are useful for logging, early stopping, and other tasks. ","category":"page"},{"location":"api/callbacks/","page":"Callbacks","title":"Callbacks","text":"Callbacks are passed to the Trainer constructor: ","category":"page"},{"location":"api/callbacks/","page":"Callbacks","title":"Callbacks","text":"callback1 = Checkpointer(...)\ntrainer = Trainer(..., callbacks = [callback1, ...])","category":"page"},{"location":"api/callbacks/","page":"Callbacks","title":"Callbacks","text":"Callback implement their functionalities thanks to the hooks described in the Hooks section of the documentation.","category":"page"},{"location":"api/callbacks/#Available-Callbacks","page":"Callbacks","title":"Available Callbacks","text":"","category":"section"},{"location":"api/callbacks/","page":"Callbacks","title":"Callbacks","text":"A few callbacks are provided by Tsunami.","category":"page"},{"location":"api/callbacks/#Checkpoints","page":"Callbacks","title":"Checkpoints","text":"","category":"section"},{"location":"api/callbacks/","page":"Callbacks","title":"Callbacks","text":"Callbacks for saving and loading the model and optimizer state.","category":"page"},{"location":"api/callbacks/","page":"Callbacks","title":"Callbacks","text":"Tsunami.Checkpointer\nTsunami.load_checkpoint","category":"page"},{"location":"api/callbacks/#Tsunami.Checkpointer","page":"Callbacks","title":"Tsunami.Checkpointer","text":"Checkpointer(folder = nothing) <: AbstractCallback\n\nAn helper class for saving a FluxModule and the fit state. The checkpoint is saved as a JLD@ file with the name ckpt_epoch=X_step=Y.jld2. A symbolic link to the last checkpoint is also created as ckpt_last.jld2.\n\nA Checkpointer is automatically created when checkpointer = true is passed to fit!.\n\nIf folder is not specified, the checkpoints are saved in a folder named checkpoints in the run directory.\n\nSee also: load_checkpoint.\n\nExamples\n\ncheckpointer = Checkpointer()\nTsunami.fit!(..., callbacks = [checkpointer])\n\n\n\n\n\n","category":"type"},{"location":"api/callbacks/#Tsunami.load_checkpoint","page":"Callbacks","title":"Tsunami.load_checkpoint","text":"load_checkpoint(path)\n\nLoads a checkpoint that was saved to path.  Returns a namedtuple containing the model state, the fit state, the lr schedulers and the optimisers.\n\nSee also: Checkpointer.\n\nExamples\n\nckpt = load_checkpoint(\"checkpoints/ckpt_last.jld2\")\nmodel = MyModel(...)\nFlux.loadmodel!(model, ckpt.model_state)\n\n\n\n\n\n","category":"function"},{"location":"api/callbacks/#Writing-Custom-Callbacks","page":"Callbacks","title":"Writing Custom Callbacks","text":"","category":"section"},{"location":"api/callbacks/","page":"Callbacks","title":"Callbacks","text":"Users can write their own callbacks by defining customs types and implementing the hooks they need. For example","category":"page"},{"location":"api/callbacks/","page":"Callbacks","title":"Callbacks","text":"struct MyCallback end\n\nfunction Tsunami.on_train_epoch_end(cb::MyCallback, model, trainer)\n    fit_state = trainer.fit_state # contains info about the training status\n    # do something\nend\n\ntrainer = Trainer(..., callbacks = [MyCallback()])","category":"page"},{"location":"api/callbacks/","page":"Callbacks","title":"Callbacks","text":"See the implementation of Checkpointer and the  Hooks section of the documentation for more information on how to write custom callbacks. Also, the examples folder contains some examples of custom callbacks. ","category":"page"}]
}
